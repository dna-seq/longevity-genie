Distributed System Executions

Distributed systems pose unique challenges for software developers. Understanding the system's communication topology and reasoning about concurrent activities of system hosts can be difficult. The standard approach, analyzing system logs, can be a tedious and complex process that involves reconstructing a system log from multiple hosts' logs, reconciling timestamps among hosts with non-synchronized clocks, and understanding what took place during the execution encoded by the log. This article presents a novel approach for tackling three tasks frequently performed during analysis of distributed system executions: (1) understanding the relative ordering of events, (2) searching for specific patterns of interaction between hosts, and (3) identifying structural similarities and differences between pairs of executions. Our approach consists of XVector, which instruments distributed systems to capture partial ordering information that encodes the happens-before relation between events, and ShiViz, which processes the resulting logs and presents distributed system executions as interactive time-space diagrams. Two user studies with a total of 109 students and a case study with 2 developers showed that our method was effective, helping participants answer statistically significantly more system-comprehension questions correctly, with a very large effect size.

Additional Key Words and Phrases: Distributed systems, program comprehension, log

Ivan Beschastnikh, Perry Liu, Albert Xing, Patty Wang, Yuriy Brun, and Michael D. Ernst. 2020. Visualizing Distributed System Executions. ACM Trans. Softw. Eng. Methodol. 29, 2, Article 9 (March 2020), 38 pages. https://doi.org/10.1145/3375633

We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) through the Discovery grant, funding reference numbers 2014-04870 and 2019-05090. Authors Perry Liu, Albert Xing, and Patty Wang contributed to the project while sponsored by the NSERC USRA program. This material is based upon work supported by the United States Air Force under contract nos. FA8750-12-2-0107 and FA8750-15-C-0010, and by the National Science Foundation under grant nos. CCF-1453474 and CCF-1763423.

Authors' addresses: I. Beschastnikh, P. Liu, A. Xing, and P. Wang, University of British Columbia, 201-2366 Main Mall, Vancouver, BC, V6T 1Z4, Canada; emails: bestchai@cs.ubc.ca, {perry, albert.xing}@alumni.ubc.ca, patty.pcw@gmail.com; Y. Brun, University of Massachusetts Amherst, 140 Governors Drive, Amherst, MA, 01003-9264; email: brun@cs.umass.edu; M. D. Ernst, University of Washington, 185 Stevens Way, Seattle, WA, 98195-2350; email: mernst@cs.washington.edu.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise or republish, to post on servers or to redistribute to lists requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

2020 Copyright held by the owner/author(s). Publication rights licensed to ACM. 1049-331X/2020/03-ART9 $15.00

INTRODUCTION

Understanding and debugging distributed systems is challenging. Given two events at different hosts, it is not obvious whether one of them is causally dependent on the other, even if each of the events has a timestamp. Distributed systems are prone to failure and are designed to be resilient to it: messages may be dropped, new hosts may join, and existing hosts may leave or fail without notice. Pausing or stepping through a distributed execution is generally impossible.

Today, one standard strategy that developers use to diagnose software bugs and reason about program execution is logging [20, 40, 88, 123, 124]. Prior work has observed that logs play an important role in resolving cloud-based system outages [50, 51, 122], access-denied issues [120], and configuration issues [121], among other tasks [85]. A typical way that logging is used in distributed systems is by logging system behavior to generate a log for each host (e.g., using printf statements or logging libraries such as Log4J). The developers then analyze the global sequence of events across different hosts by serializing the logs from multiple hosts using timestamps in the logs. Howeverproduces a textual log of printf-style messages augmented with vector clock timestamps. ShiViz reads these logs to reconstruct the graph of inter-host communication and display a time-space diagram. ShiViz includes specific capabilities to help developers implement correct systems. The capabilities help a developer understand event ordering, query for interaction patterns, and compare executions. We evaluated the behavior-understanding capabilities in ShiViz through three studies: 

1. We ran a controlled experiment with a mix of undergraduate and graduate participants. One group of participants studied distributed system executions using ShiViz and the other group without ShiViz. The study asked all participants to answer questions about the system represented by the executions. 
2. Students in a distributed systems course used ShiViz as part of two homework assignments to help them debug and understand their implementations.
3. We ran a case study with two systems researchers who were developing complex distributed systems to evaluate the end-to-end usefulness of ShiViz to developers in their work.

Across these studies, we collected the developers' impressions via surveys. Our evaluation results demonstrate that ShiViz supports both novice and advanced developers in distributed system development tasks. For example, our controlled experiment with 39 participants showed that those using ShiViz answered statistically significantly more distributed system understanding questions correctly than control-group participants without ShiViz, with a very large effect size. The two case studies provide qualitative data about the ShiViz developer experience, indicating that ShiViz helped these participants solve their problems faster than if they were to use other tools.

This article's main research contributions are:
1. A new method (and the supporting open source implementation for systems written in C, C++, Java, and Go, as detailed below) for logging and analyzing distributed systems to support common system understanding developer tasks.
2. Advanced, composable graph transformations for manipulating distributed system execution graphs, including constrained custom structured search and filtering by process.
3. A mechanism for side-by-side juxtaposition of pairs of execution graphs, supporting all of the single graph transformations, as well as new transformations to highlight graph differences and similarities.
4. Algorithms to cluster distributed executions using two approaches computed over sets of graphs: clustering by similarity to a specified graph and by number of processes.

For simplicity, we refer to any one of the libraries as XVector. ACM Transactions on Software Engineering and Methodology, Vol. 29, No. 2, Article 9. Pub. date: March 2020.

ShiViz, a robust, web-deployed, freely available implementation of the above contributions for developers to use for distributed system development. ShiViz is available online: http://bestchai.bitbucket.io/shiviz/. A video demonstrating key features of ShiViz is also available: http://bestchai.bitbucket.io/shiviz-demo/. ShiViz is being actively used in the research community and by projects within companies like Microsoft and by popular open source projects like Akka.

Four implementations of XVector, easy-to-use libraries for logging distributed system executions, for C, C++, Java, and Go. These libraries are available online: https://github.com/

The rest of this article is structured as follows: Section 2 illustrates distributed systems challenges with three use cases. Section 3 presents distributed systems background. Section 4 presents ShiViz's mechanisms for navigating and manipulating the visualization, which address the challenges of distributed system understanding. Section 5 describes our XVector and ShiViz implementations. Section 6 evaluates XVector and ShiViz with a series of controlled experiments and user studies. Section 7 discusses the threats to the validity of our study. Section 8 describes the work's limitations and future work. Section 9 places our work in the context of related research.

EXAMPLE USE CASES
This section presents three use cases that highlight how ShiViz supports developers in debugging distributed systems. These use cases were inspired by our own experiences in building distributed systems, as well as by conversations with real distributed system developers.

1. A developer attempts to understand why two leader hosts are active simultaneously in her implementation of a leader election algorithm.
A safety invariant in a leader election algorithm is that at any given time, there is at most one leader in the system. If a leader fails (such as crashing or becoming disconnected from the network), then a new leader is elected. Consider a developer whose implementation sometimes generates a situation in which two leaders are active simultaneously. The developer wants to understand under what circumstances this situation occurs.
It is standard practice for the developer to add print statements to capture critical state transitions of the system. For example, when a host becomes the leader, the host logs a message to that effect. To use ShiViz, the developer takes three additional actions: (1) The developer instruments the system using the XVector library, which augments each logged message generated by the system with a vector timestamp. (2) The developer deploys the instrumented systemcorrectly spelled and formatted.

issues in these three use cases without ShiViz, a developer today would (1) add logging code, such as print statements, automatically or manually to the system, and (2) study the resulting textual logs, one per node in the system. As a result, without ShiViz, the developer cannot easily understand the partial ordering of concurrent events in the system and has to piece together what happened by scanning through the individual node logs. Section 9 considers specific tool alternatives and prior research work in more detail. 

Summary. These three use cases illustrate the utility of ShiViz's mechanisms for (1) understanding the relative ordering of events in an execution, (2) querying for patterns of interaction between hosts in an execution, and (3) identifying structural similarities and differences between pairs of executions.

BACKGROUND: DISTRIBUTED SYSTEMS

This section overviews distributed systems concepts necessary to understand our work. A distributed system is composed of a number of hosts, and each host generates a totally ordered sequence of events. Each event is associated with a set of attributes, such as the time the event occurred. A host trace is the set of all events generated at one host.

Order is an important property of a distributed execution. Events are ordered in two ways. First, the host ordering orders every pair of events at the same host, but does not order events that occur at different hosts. Second, the interaction ordering orders dependent events at different hosts; for example, if two hosts use message passing to communicate, a send message event is ordered before the receive message event for the same message. Taken together, the host and interaction ordering generate a partial ordering over all events in the execution. The partial ordering is known as the happens-before relation [68]. Understanding this partial order is central to most tasks that a distributed system developer performs.

Figure 1 is a time-space diagram, which is the standard visualization of the happens-before relation in a distributed system [66]. (ShiViz displays distributed system executions as time-space diagrams; see Figure 2.) The time-space diagram expresses happens-before relations as directed edges between events. (This ordering is transitive, but for clarity, the diagram omits transitive edges.) For example, in the diagram, the tx prepare event at TX Manager occurs before the abort event at Replica 1 (there is an edge from tx prepare to abort). In a time-space diagram, two events are concurrent if the time-space diagram lacks a directed path between them. For example, in Figure 1 there is no directed path between the concurrent events abort at Replica 1 and commit at Replica 2.

One way to represent partial order in a system trace is to associate a vector clock timestamp with each event. These timestamps make explicit the partial order of events in the system trace. The remainder of this section briefly explains vector time and an algorithm to record vector timestamps in a distributed system [36, 80]. Though more efficient vector clock mechanisms exist [4, 6], we believe that vector timestamps [36, 80] are practical for debugging: short time periods on large systems, or during development and testing.

For concreteness, our explanation below assumes that the distributed system uses message passing, but vector timestamps are equally applicable to a system that uses other mechanisms for inter-host communication, such as shared memory.

ACM Transactions on Software Engineering and Methodology, Vol. 29, No. 2, Article 9. Pub. date: March 2020.

Fig. 1. Time-space diagram illustrating an execution of the two-phase commit protocol [8] with one transaction manager and two replicas. The vertical lines represent the host ordering, and the diagonal lines represent the interaction ordering. Each event has an associated vector timestamp. For conciseness, this time-space diagram does not explicitly include message send and receive events.

Ordering Events with Vector Time

In a distributed system of h hosts, each host maintains a local logical time. In addition, each host maintains a vector clock, which is an underestimate of the logical time at all hosts. A vector clock = is an array of monotonically increasing clocks C [c 0, c 1, . . . ,c h1], where c j represents the local logical time at host j. C denotes the vector clock at host i and C [j] represents its current knowledge about the local logical time at j.

The hosts update their clocks to reflect the execution of events in the system by following:

(  ) Each host starts with an initial vector clock [ , . . . , ].
(  ) After a host i generates an event, it increments its own clock value (at index i) by , i.e., ++ C [i] . Sending and receiving a message are each considered an event.
(  ) Every message sent by a host i includes the value of its vector clock C. Upon receiving the message,Software Engineering and Methodology, Vol. 29, No. 2, Article 9. Pub. date: March 2020.

The execution graph in ShiViz is accessible in the left panel and is linked to the execution graph through visual cues. Hovering over an event or clicking on an event in the graph highlights the background of the corresponding log line in the left panel. This informative feedback supports the developer's mental model that the execution graph is a more structured representation of the log. For example, the selected event in Figure 2 corresponds to the highlighted log line "Closed, exiting". Likewise, clicking on a log line on the left highlights "The log lines are positioned to horizontally align with their corresponding events to strengthen the association". However, this also means that the log lines may appear in an order different from the order of log lines in the input log but always one consistent with the partial order and one that could have been generated by the same execution.

4.1.2 Associating Log Lines with Hosts

To make it easy to spot the lines in the log associated with a specific host, each host in the execution graph is associated with a color that is used for its host box, event circles, and log lines. This color-coding works best when displaying few enough hosts to allow ShiViz not to reuse host colors.

4.1.3 Graph Transformations

The execution graph may contain a large number of events, including ones that are not relevant to a developer's task. ShiViz includes three graph transformations shown in Figure 3 to help developers focus on relevant behavior in the execution.

Transformation 1: collapse local events

ShiViz emphasizes ordering information and communication between processes. Series of local events that do not involve interactions with other processes are often less relevant to understanding the communication patterns in the system. ShiViz groups and merges these local events into a single node to simplify the visualization and emphasize global ordering information, as illustrated in Figure 3(a). This transformation is enabled by default: All series of events that can be collapsed are collapsed when the execution graph is first shown to the developer. They can expand or collapse each set of nodes independently by clicking on the node and selecting expand or collapse.

Transformation 2: hide host timeline

A developer may want to focus on a subset of the hosts in the system. Using the hide host transformations, a developer can remove a host (including the corresponding host timeline and log lines) from view or bring these back into view. To hide a host, a developer double-clicks on the host box at the top of the time-space diagram. Hidden hosts are collected in the right panel (in Figure 2). To unhide the host, developers double click on the host box in this panel. A dashed edge connects two events at not-hidden hosts that are connected transitively through one or more hidden hosts. Figure 3(b) illustrates this.

Transformation 3: filter by host

The filter by host transformation refines the graph to show just the set of events and hosts that are relevant to communication with a particular host. A white square inside the host box denotes a filtered host. It is possible to filter by more than one host and to unfilter hosts in any order. Figure 3(c) illustrates the filter by host transformation. Figure 4 shows an example of this transformation on the log and view from Figure 2.

Throughout these transformations, the log in the left panel continues to reflect the visible events on the right. For example, when a host is hidden from the graph, the log lines for that host are hidden as well. Therefore, these three graph transformations are log transformations as well.

4.1.4 Ordering Hosts

ShiViz allows users to order the hosts in one of four ways: (1) Order hosts in descending order of the number of events each host generated in total. (2) Order hosts in ascending order of the number of events each host generated in total. (3) Order hosts in descending order of the line number where the host appeared in the log. (4) Order hosts in ascending order of the line number where the host appeared in the log.

4.2 Supporting Execution Graph Queries

Few developers read or browse a complex system's log manually. Instead, developers often focus on specific parts of the log and write regular expressions to parse information they need out of the log [109]. Presenting a visual partial ordering that describes an execution cannot help with this. To be useful for complex logs, ShiViz must be able to support developer queries about the partial ordering. A key challenge for ShiViz is in supporting graph-based queries. Using graph-based queries, developers can express the topologies of interesting event orderings, rather than the text contents of the events themselves.

ShiViz implements two kinds of searches: structured search and keyword search. When a developer clicks in the search area at thestudy, a case study, and a user survey. In the controlled study, participants were asked to complete four tasks related to understanding and debugging a distributed system using either ShiViz or a text-based log analysis tool. The results showed that participants using ShiViz completed the tasks faster and with fewer errors than those using the text-based tool. In the case study, ShiViz was used to analyze logs from a complex distributed system, resulting in significant time savings and improved understanding of the system's behavior. The user survey collected feedback from 13 participants who had used ShiViz in their work. The feedback was overwhelmingly positive, with users reporting that ShiViz was easy to use, helped them understand their systems better, and saved them time in debugging.

Overall, our evaluation shows that ShiViz is an effective tool for understanding and debugging distributed systems. Its search and visualization capabilities allow developers to quickly identify patterns and anomalies in system behavior, while its support for multi-execution comparison and clustering makes it easy to compare and analyze multiple executions. ShiViz's implementation as a pure client-side web application also makes it highly portable and easy to deploy in corporate environments. We believe that ShiViz has significant potential to improve the efficiency and effectiveness of distributed systems development and debugging.Experiment, via a study with 70 students using ShiViz in a distributed systems class, and via two case studies, in which one developer per study used ShiViz while working on a complex system. We also evaluated XVectors performance overhead. We studied four research questions, which are based around the design features of ShiViz highlighted in Section 4:

RQ1: Does ShiViz help understand the relative ordering of events?
RQ2: Does ShiViz help query for inter-host interaction patterns?
RQ3: Does ShiViz help identify structural similarities and differences between pairs of executions?

To evaluate RQ1-RQ3, we designed a controlled experiment with 39 participants. We first designed a 15-question questionnaire (Figure 9) about four distributed executions of three systems: (1) a reliable broadcast protocol (RBcast), (2) a Facebook client-server interaction, (3) an enterprise distributed system called Voldemort [112] (Vold), and (4) a pair of Facebook interactions (FB pair). Figure 2 shows a visualization of the Voldemort log (with some hidden from view). These questions were based on our experience teaching a fourth-year distributed systems course at the University of British Columbia. In this course, the students work in teams of 24 students to build complex distributed systems in Go. As they develop their systems, the students frequently run into debugging challenges and ask questions in office hours and in an online discussion forum.

The questions in our study build on the student questions in the course, combined with our expertise as educators aimed to design questions that accurately measure system comprehension. A control group of 15 participants relied on the raw logs to answer the questions. These logs contained vector timestamps. We used raw logs as our baseline because we know of no other log analysis tool for understanding communication patterns in distributed systems. Our baseline is what a professional developer would use today [20, 40, 85]. A treatment group of 24 participants received a 10-minute introduction to ShiViz, then used the same logs and the ShiViz tool to answer the questions. None of the participants in the treatment group had prior exposure to ShiViz. Participants in both the control and treatment groups were limited to 60 minutes. The participants were assigned to the treatment and control groups at random; this randomness led to the uneven split.

The controlled study included a mix of six graduate students, eight undergraduate students, and one professor. The treatment study included 24 students in a combined graduate and undergraduate course: eight graduate students and 16 undergraduate students. Figure 10 reports the demographics of the two groups. The participants in both groups were non-experts in the field of distributed systems, though about 50% had prior experience with such systems, and about 60% had taken a related course. All participants were students or employees at UMass Amherst (and none are authors of this article).

For each group and for each question, Figure 9 reports the percentage of participants who answered the question correctly, and the Fisher's exact test p-value showing whether there is a statistically significant difference between the treatment and control groups.

Overall effect of using ShiViz: First, we compared the distribution of correctly answered questions by the treatment group to that of the control group using the two-tailed unpaired t-test. The test rejected the null hypothesis that these groups came from the same distribution (p < 0.00002). We thus conclude that using ShiViz did significantly impact the participants' ability to answer distributed systems questions correctly. The effect size of the impact is very large (Cohen's d = 1.56). We then, for each question, compared the distributions of correct answers and measured if ShiViz's effect was statistically significant for that question. We used the non-parametric two-tailed Fisher's exact test that makes no assumptions about the underlying distribution and is recommended for small sample sizes. We found that for six of the fifteen questions, the test confirmed that the treatment group and control group were statistically significantly different (p < 0.05, highlighted in Figure 9). For each of these six questions, participants using ShiViz answered more questions correctly than the participants in the control group. For these six questions, on average, 52% more of the participants using ShiViz answered questions correctly than participants in the control group.

UI: Diagram comprehension: The treatment participants scored 83%-92% (mean 90%) on questions that tested the participants' understanding of the visual components making up the execution graph as well as the relation of these components to information captured in the input log. This was an improvement over the control group, which scored 47%-100% (mean 73%). On some questions, such as question 1.1, the control group did better. We believe the reason for this is the lack of experience with ShiViz. To answer questionAnd, Fisher's exact test shows that for 6 of the 15 questions, there was a statistically significant difference (p < 0.05) in how the two groups performed. 
Developing Distributed Systems
6.2.1 Distributed Systems Course Study.
We conducted a study using an undergraduate distributed systems course in which 70 participants used ShiViz. None of the authors were directly involved in the instruction of this course. The course instructor presented vector clocks in a lecture and then briefly demonstrated how ShiViz can be used to visualize a log. The students were then given two homework assignments that described the file format ShiViz uses. One assignment asked the students to implement the bully leader election algorithm [16], and the other the distributed two-phase commit transaction algorithm [8]. As part of the assignments, the students were asked to write a short description of their experience using ShiViz within the assignments. Once the course ended, the instructor shared the student's assignments with us. More information about the two assignments is available online [13].

Leader election assignment.
For the first assignment, the participants were required to implement a bully leader election algorithm, cause it to exhibit some interesting behavior, capture that behavior in logs, and then explain that behavior. Many participants described how ShiViz helped them to discover, understand, and fix defects, then confirm correct behavior, in their implementation of the bully leader election algorithm. The leader election system should be robust against environmental faults such as network partitions, dropped packets, failing hosts, and slow communication links, but the initial implementations were flawed. The visualization helped us spot a bug that nodes were not sending their most current timestamps to other nodes until a COORD is sent. [Group 28] (Section 4.1.1). One specific instance of how ShiViz helped us debug our program was when we were struggling to understand why our timestamps seemed to get increasingly misaligned over time. It was difficult to understand the ordering of our events through our Linux console, so we used ShiViz to understand the...we were noticing a problem where the second-highest node regularly set itself as the coordinator, even without any network failures. The highest node was also listing itself as a coordinator. We discovered this by examining the visualization and noticing that both the two highest nodes were often simultaneously listed as coordinators. We used this visualization to help us determine the source of...

Two-phase commit assignment.
For the second assignment, the participants were required to implement a system based on the two-phase commit protocol, cause their system to complete a transaction with no errors, and then cause a transaction that commits despite the failure of an ACM Transactions on Software Engineering and Methodology, Vol. 29, No. 2, Article 9. Pub. date: March 2020. In their reports, the participants frequently referred to the helpfulness of the visualization in confirming behavior: these ShiViz logs show a clear indication of successful functionality of the basic 2-phase commit protocols. [Group 18]. Some groups also noted that ShiViz helped them understand concurrency in their algorithm: ShiViz was very useful for visualizing these parallel transactions. [Group 15]. In both assignments, participants noted the importance of logging key events in the system because otherwise, the events are invisible to ShiViz. They also mentioned that ShiViz diagrams can be complex, particularly in cases with significant concurrency between the hosts.

6.2.2 Case Studies with Two Researchers.
We performed two case studies, each one with a highly experienced systems researcher who used ShiViz to understand and debug a complex system they were working on at the time. The first researcher used ShiViz for four months while designing and debugging a distributed system for troubleshooting other systems [108]. The second researcher used ShiViz during the course of two weeks in debugging two locking implementations in the context of a commercial multi-threaded key-value store that is used as a storage engine for MongoDB [82]. In both cases, the researchers learned and used ShiViz on their own.

The system was to minimize input sequences to distributed systems when reproducing failures; that is, his system tries to minimize external events and internal event schedules when recreating a failed execution. In building this system, the researcher used ShiViz to debug and understand behavior across a variety of distributed systems. We performed a semi-structured interview with the researcher after the four-month period to gain insight into how he used ShiViz and what features he found most useful.

The researcher initially used ShiViz to come up with a heuristic for the minimization algorithm. Instead of enumerating the entire event schedule space to find the minimal schedule, he wanted to inspect several schedules that led to the same bug and then attempt to find unifying features among these schedules. He did this by using ShiViz to visually compare (Section 4.3.1) several event schedules from three different distributed systemsOne thread releasing the lock and another thread acquiring that lock were non-uniform. This was contrary to the assumption that each time the acquiring thread is spinning, as threads are sent the wake signal. The problem was that a thread may take a while to wake up: it might not wake up early enough to enter the spinning state when the lock becomes available. This explained why the lock wait times were longer than expected. In addition to the acquisition time disparity, the researcher used ShiViz to determine that (1) one thread does different work than the others (based on the amount of time spent in this thread). By examining the code, the researcher realized that this thread is a work producer. (2) Using ShiViz, the researcher realized that the pthread mutex is unfair relative to the ticket-based lock; some threads acquire the lock more often than others when using the pthread mutex, which creates a priority inversion. And (3) the priority inversion makes the work producer thread run more often when using the pthread mutex than when using the ticket lock. Since the work producer is the bottleneck in the studied workload, the workload ends up completing faster with pthread.

In summary, the researcher noted that ShiViz helped her solve the problem faster than if she used other tools: "I seriously doubt that I would have been able to pinpoint the precise cause of the problem without ShiViz, and even if I had, it would have taken me many weeks."

Our XVector vector clock instrumentation libraries have (1) a network overhead due to additional vector clock timestamps in network payloads, and (2) a vector clock logging overhead. We evaluate both of these overheads in the GoVector library, which is an XVector library for Go lang. Our experience using other XVector libraries in teaching and with open-source systems indicates that other XVector libraries have similar overheads. We use GoVector to instrument etcd, a popular distributed key-value store that uses the Raft consensus protocol [87] to achieve the Experimental setup. We ran all experiments on an Intel machine with a 4-core i5 CPU and 8 GB of memory. The machine was running Ubuntu 14.04, and all applications were compiled using Go 1.6 for Linux/amd64. We exercised etcd by loading it using the Yahoo! Cloud Serving Benchmark (YCSB-B) with a uniform distribution (50% gets and 50% puts on random keys). The etcd Raft cluster consisted of three replication nodes. We measured the latency and goodput on both the uninstrumented and the instrumented versions of etcd. In both experiments, the maximum number of clients that could execute concurrently without reaching maximum CPU utilization was 72.

Identity and the other are the node's logical clock timestamp. The overhead of vector clocks is fore a product of the number of nodes interacting during execution and the number of 64-bit node's messages. In practice, adding vector clocks to messages slows down each, which can impact the behavior of the system. Figure 11 shows the goodput for uninstrumented and instrumented Raft with each point representing an average over three executions. Adding vector clocks to Raft slowed down the broadcast of heartbeat messages and caused an increase in goodput with 36 clients. As the number of clients grew, the size of the vector clocks overcame this goodput saving. Across these experiments, goodput decreased by 13% on average, and the maximum decrease was 23%.

GoVector imposes a latency overhead in computing vector clocks and in serializing and deserializing messages with vector clocks. Figure 12 shows Raft's latency processing client requests in the uninstrumented and instrumented versions of Raft. Across these experiments, the latency increase was 7% on average with a maximum increase of 21%.

In the above experiment, we also collected information on logging overhead. We found that the average execution time of a single logging statement is 20 microseconds. In our local area network with a round trip time of 0.05 milliseconds while running etcd with 1-second timeouts, we were able to execute approximately 50K logging statements per node before perturbing the system.

The overhead imposed by XVector makes it a viable tool during development but not in production. We believe that existing distributed tracing systems, such as X-Trace [39] and Dapper [110], which are intended for production use, can be extended with tracking of vector-clock timestamps. We have also successfully reconstructed ShiViz-compatible logs from X-Trace traces.

ShiViz is designed to run in the browser, making it more accessible and easier to use. However, design choice does impact its ability to scale to large and complex logs. In this section, we report scalability results from using ShiViz on a variety of logs. In each experiment, we report the time to load and visualize the log with ShiViz in a browser. In every logUsable and reasonably performant tools are necessary for complex distributed systems. The XVector and ShiViz tools have been introduced in a practitioner-oriented article [14] and have been used by research groups, developers, and companies. ShiViz is employed in research projects for debugging distributed systems [82, 107], and it is also used by P and P# projects within Microsoft to visualize traces of executions during debugging.13 Akka is a popular toolkit for building concurrent and distributed systems using actor-based programming in Java and Scala. Akka includes scripts for developers to convert their logs into ShiViz format.14 TLC is a model checker for TLA+ and PlusCal modeling languages that supports ShiViz for visualizing the counter-example trace from a model checking run of a concurrent model.15 The Bro network monitor also includes ShiViz support to help developers visualize distributed network threats.

THREATS TO VALIDITY
Salman et al. [102] and Pham et al. [89] have demonstrated that conclusions based on studies using students can generalize to the broader developer community. We evaluated ShiViz with 70 participants in an undergraduate distributed systems course with fourth-year CS majors at the University of British Columbia, a major research university (Section 6.2.1). Most of these became novice professional software developers only a few months later; we expect our results to generalize at least to novice engineers and perhaps beyond. 

While the goal of our ShiViz evaluations was to evaluate a set of user interface features and underlying algorithms, such evaluations and interface design are inherently iterative. The developers' interactions with the tool inform the tool makers and suggest new interface features. For example, our studies revealed that developers might benefit from highlighting differences between two executions, as described in Section 4.3.1. We elected to implement this feature as part of ShiViz and include the description of this finding here, even though the ShiViz version used for evaluation did not yet include this feature. ShiViz displayed executions side-by-side but did not highlight differences. Adding such features could, in theory, alter our results, although the effect is unlikely.

Our study asked participants to answer questions (Figure 9) about the system's behavior. The goal of the questions is to gauge the correctness of the participants' understanding of the system behavior. We used our expertise as educators to design questions that accurately measure system comprehension. More questions would likely have resulted in more accurate results, but the study had to balance this accuracy against the length of the study.

DISCUSSION
ShiViz surfaces low-level ordering information, which makes it a poor choice for understanding aggregate system behavior. The ShiViz visualization is based on logical and not real-time ordering and cannot be used to study certain performance characteristics. The ShiViz tool is implemented as a client-side-only browser application, making it portable and appropriate for analyzing sensitive data.

ShiViz reconstructs the happens-before relation using vector timestamps associated with logged events. The protocol for maintaining these timestamps (detailed in Section 3.2) does not provide ordering information when messages are lost. In particular, a send event without a corresponding receive event will not be identified as a send event. This is because the event that immediately happens-after the send event cannot be associated with a remote host since the sent message was lost.

We are working on optimizing visualization layout by formulating an algorithm that can produce a host ordering that minimizes the total number of line crossings (this problem is NP-hard). We envision several additional features to make ShiViz even more useful to developers, such as real-time online visualizations and linking the visualization with code.

RELATED WORK
There are many log analysis tools for processing totally ordered logs, including Fluentd [38], Logstash [76], Graylog [47], Splunk [111], Papertrail [87], Loggly [75], Sumologic [113], and Zinsight [26, 27]. A common enterprise solution for end-to-end log analysis is the ELK stack for indexing and storing traces (Elasticsearch [34]), filtering and extracting data from traces (Logstash [76]), and visualization of the results (Kibana [60]). These tools can aid manual performance analysis and debugging [26]. 

Visualizing Distributed Systems
The most closely related systems to ShiViz are Poet [65] and DTVS [33]. Poet is a toolchain for instrumenting and then visualizing distributed systems as time-space diagrams. DTVS visualizes concurrency regions in a trace of synchronous distributed system executions. ShiViz makes several technical advances over both Poet and DTVS, including graph transformations (e.g., filter by host), keyword and structured search, multi-execution comparison, and clustering. ShiViz is also simpler to use and deploy: ShiViz does not require a server, runs in a browser, and requires nodistributed system executions, we have also discussed related work in the field. We have highlighted the strengths and weaknesses of various tools and techniques, including those that focus on performance analysis, tracing and debugging, summarizing logs, and visualizing distributed systems in integrated development environments. 

We have also identified areas where our method could be extended or complemented by other techniques, such as using natural language processing to reconstruct distributed workflow information from existing logs or incorporating techniques for managing long executions like aggregating events or data, visualizing heatmaps, bundling edges, or animating the visualization.

Overall, our method provides a valuable tool for developers to gain insight into the behavior of their distributed systems. By automating logging and providing a visual representation of the partial ordering of events in executions, ShiViz helps developers detect issues with their systems and improve their design and performance.practices in Java-based open source software projects: A replication study in Apache Software Foundation. Empirical Software Engineering, Vol. 26, No. 1, pp. 1-34. DOI: https://doi.org/10.1007/s10664-020-09820-9.

In order to enhance distributed system executions, we have developed XVector as a collection of libraries that can add partial ordering information to execution logs, and ShiViz as a web-based tool. All of our tools, experimental designs, and data are publicly available. The source code for XVector libraries is available in ACM Transactions on Software Engineering and Methodology, Vol. 29, No. 2, Article 9 (March).

We would like to thank Jenny Abrahamson for creating the initial prototypes of XVector and ShiViz, Vaastav Anand for his help in evaluating ShiViz's scalability and assisting with the development of several XVector variants, Graham St-Laurent and Matheus Nunes for their contributions to improving the ShiViz implementation, Donald Acton and Colin Scott for their evaluation of ShiViz, and all participants in our user studies.

REFERENCES

[1] Abrahamson, J., Beschastnikh, I., Brun, Y., & Ernst, M. (2013). Shedding light on distributed system executions. In Proceedings of the International Conference on Software Engineering (ICSE) Poster Track.

[2] Acharya, M., Xie, T., Pei, J., & Xu, J. (2008). Mining API patterns as partial orders from source code: From usage scenarios to specifications. In Proceedings of the European Software Engineering Conference and ACM SIGSOFT International Symposium on Foundations of Software Engineering (ESEC/FSE).

[3] Aguilera, M. K., Mogul, J. C., Wiener, J. L., Reynolds, P., & Muthitacharoen, A. (2007). Performance debugging for distributed systems of black boxes. SIGOPS Operating Systems Review, 41(5), 74-86.

[4] Almeida, P.S., Baquero, C., & Fonte, V. (2008). Interval tree clocks. In Proceedings of the International Conference on Principles of Distributed Systems (OPODIS).

[5] Barham, P., Donnelly, A., Isaacs, R., & Mortier, R. (2004). Using Magpie for request extraction and workload modelling. In Proceedings of the Symposium on Operating Systems Design & Implementation (OSDI).

[6] Becker, D., Rabenseifner, R., Wolf, F., & Linford, J.C. (2011). Scalable timestamp synchronization for event traces of message-passing applications. Parallel Computing, 37(11), 753-767.

[7] Benomar, O., Sahraoui, H., & Poulin, P. (2010). Visualizing software dynamicities with heat maps. In Proceedings of the IEEE Working Conference on Software Visualization (VISSOFT).

[8] Bernstein, P.A., Hadzilacos, V., & Goodman, N. (1987). Concurrency Control and Recovery in Database Systems.

[9] Beschastnikh, I., Brun, Y., Abrahamson, J., Ernst, M.D., & Krishnamurthy, A. (2011). Using declarative specification to improve the understanding, extensibility and comparison of model-inference algorithms. IEEE Transactions on Software Engineering.

[10] Beschastnikh, I., Brun, Y., Ernst, M.D., & Krishnamurthy A. (2014). Inferring models of concurrent systems from logs of their behavior with CSight. In Proceedings of the International Conference on Software Engineering (ICSE).

[11] Beschastnikh, I., Brun, Y., Ernst, M.D., Krishnamurthy, A., & Anderson, T.E. (2015). Mining temporal invariants from partially ordered logs. SIGOPS Operating Systems Review, 49(1), 53-66.

[12] Beschastnikh, I., Brun, Y., Schneider, S., Sloan, M., & Ernst, M.D. (2016). Leveraging existing instrumentation to automatically infer invariant-constrained models. In Proceedings of the ACM SIGSOFT Conference on the Foundations of Software Engineering (FSE).

[13] Beschastnikh, I., Liu, P., Xing, A., Wang, P., Brun, Y., & Ernst, M.D. Shviz evaluation details. Retrieved from http://bestchai.bitbucket.io/shiviz-evaluation/.

[14] Beschastnikh, I., Wang, P., Brun, Y., & Ernst, M.D. (2014). Debugging distributed systems. Communications of the ACMPractices in Java-based open source software projects: A replication study in Apache Software Foundation. Empirical Software Engineering, (Feb.), DOI: https://doi.org/10.1007/s10664-018-9653-7 [1]

Mariano C. Consens, Masum Z. Hasan, and Alberto O. Mendelzon. Debugging distributed programs by visualizing and querying event traces. In Proceedings of the Conference on Applications of Databases, Vol. [2]

Jonathan E. Cook and Alexander L. Wolf. Discovering models of software processes from event-based data. ACM Transactions on Software Engineering Methodology, (Oct.), DOI: https://doi.org/10.1145/3304221 [3]

Charlie Curtsinger and Emery D. Berger. Coz: Finding code that counts with causal profiling. In Proceedings of the Symposium on Operating Systems Principles (SOSP), DOI: https://doi.org/10.1145/2815400.2815409 [4]

Wim De Pauw and Henrique Andrade. Visualizing large-scale streaming applications. Information Visualization, (Apr.), DOI: https://doi.org/10.1057/ivs.2008.29 [5]

Wim De Pauw, Henrique Andrade, and Lisa Amini. Streaight: A visualization tool for large-scale streaming applications. In Proceedings of the International Symposium on Software Visualization (SoftVis), DOI: https://doi.org/10.1145/1599476.1599493 [6]

Wim De Pauw and Steve Heisig. Visual and algorithmic tooling for system trace analysis: A case study. Operating Systems Review, (Oct.), DOI: https://doi.org/10.1145/1073970.1073973 [7]

Wim De Pauw and Steve Heisig. Zinsight: A visual and analytic environment for exploring large event traces. In Proceedings of the International Symposium on Software Visualization (SoftVis), DOI: https://doi.org/10.1145/1599476.1599498 [8]

Wim De Pauw, Sophia Krasikov, and John F. Morar. Execution patterns for visualizing web services. In Proceedings of the International Symposium on Software Visualization (SoftVis), DOI: https://doi.org/10.1145/1599476.1599496 [9]

Wim De Pauw, Mihai Letia, Bugra Gedik, Henrique Andrade, Andy Frenkiel, Michael Pfeifer, and Daby M. Sow. Visual debugging for stream processing applications. In Proceedings of the International Conference on Runtime Verification (RV), DOI: https://doi.org/10.1007/978-3-540-87406-3_20 [10]

Wim De Pauw and John M. Vlissides. Visualizing object-oriented programs with Jinsight. In Proceedings of the Workshop Ion on Object-Oriented Technology. [11]

Wim De Pauw, Joel L. Wolf, and Andrey Balmin. Visualizing jobs with shared resources in distributed environments. In Proceedings of the IEEE Working Conference on Software Visualization (VISSOFT), DOI: https://doi.org/10.1109/VISSOFT.2008.4655319 [12]

Travis Desell, Harihar Narasimha Iyer, Carlos Varela, and Abe Stephens. Overview: A framework for generic online visualization of distributed systems. Electronic Notes Theoretical Computer Science (Eclipse Technol. Exch.: X Eclipse Phenom.), DOI: https://doi.org/10.1016/j.entcs.2014.07.014 [13]

Charlie Curtsinger and Emery D. Berger. Coz: Finding code that counts with causal profiling. In Proceedings of the Symposium on Operating Systems Principles (SOSP), DOI: https://doi.org/10.1145/2815400.2815409 [14]

Wim De Pauw and Henrique Andrade. Visualizing large-scale streaming applications. Information Visualization, (Apr.), DOI: https://doi.org/10.1057/ivs.2008.29 [15]

Yu Gan, Yanqi Zhang, Dailun Cheng, Ankitha Shetty, Priyal Rathi, Nayan Katarki, Ariana Bruno, Justin Hu, Brian Ritchken, Brendon Jackson, Kelvin Hu, Meghna Pancholi, Yuan He, Brett Clancy, Chris Colen, Fukang Wen, Catherine Leung, Siyuan Wang, Leon Zaruvinsky, Mateo Espinosa, Rick Lin, Zhongling Liu, Jake Padilla, and Christina Delimitrou. An open-source benchmark suite for microservices and their hardwareDependable Systems and Networks (DSN). DOI: https://doi.org/DSN.
Matheus Nunes, Ashaya Sharma, Harjeet Lalh, Augustine Wong, Svetozar Miucin, Alexandra Fedorova, and Ivan Beschastnikh. Studying multi-threaded behavior with TSViz. In Proceedings of the International Conference on Software Engineering (ICSE Demo track).
Tony Ohmann, Michael Herzberg, Sebastian Fiss, Armand Halbert, Marc Palyart, Ivan Beschastnikh, and Brun. Behavioral resource-aware model inference. In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering (ASE).
DOI: https://doi.org/ASE.
Adam Oliner, Archana Ganapathi, and Wei Xu. Advances and challenges in log analysis. Commun. ACM, (Feb.), DOI: https://doi.org/CCM.
Diego Ongaro and John Ousterhout. In search of an understandable consensus algorithm. In Proceedings of USENIX Annual Technical Conference (ATC).
papertrailapp Retrieved from https://papertrailapp.com/.
Antonio Pecchia, Marcello Cinque, Gabriella Carrozza, and Domenico Cotroneo. Industry practices and logging: Assessment of a critical software development process. In Proceedings of the International Conference on Software Engineering (ICSE).
Raphael Pham, Stephan Kiesling, Olga Liskin, Leif Singer, and Kurt Schneider. Enablers and inhibitors of Software Engineering (FSE).
James F. Kurose and Keith W. Ross. 2012. Computer Networking: A Top-down Approach (6th ed.). Pearson.
Fabrizio Lamberti and Gianluca Paravati. 2015. VDHM: Viewport-DOM based heat maps as a tool for visually aggregating web users' interaction data from mobile and heterogeneous devices. In Proceedings of the IEEE International Conference on Mobile Services (MS15). 3340. DOI: https://doi.org/MS.
Leslie Lamport. 1978. Time, clocks, and the ordering of events in a distributed system. Commun. ACM 21, 7 (1978).
Aaditya G. Landge, Joshua A. Levine, Katherine E. Isaacs, Abhinav Bhatele, Todd Gamblin, Martin Schulz, Steve H. Langer, Peer-Timo Bremer, and Valerio Pascucci. Visualizing network traffic to understand the performance of massively parallel simulations. IEEE Trans. Vis. Comput. Graph., (Dec.), DOI: https://doi.org/TVCG.
Guillaume Langelier, Houari Sahraoui, and Pierre Poulin. Exploring the evolution of software quality with animated visualization. In Proceedings of the IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC).
DOI: https://doi.org/VLHCC.
Tien-Duy B. Le, Xuan-Bach D. Le, David Lo, and Ivan Beschastnikh. Synergizing specification miners through model fissions and fusions. In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering (ASE).
DOI: https://doi.org/ASE.
Kyu Hyung Lee, Nick Sumner, Xiangyu Zhang, and Patrick Eugster. Unified debugging of distributed systems with Recon. In Proceedings of the IEEE/IFIP 31st International Conference on Dependable Systems & Networks (DSN).
DOI: https://doi.org/DSN.
Youn Kyu Lee, Jae Young Bang, Joshua Garcia, and Nenad Medvidovic. VA: A visualization and analysis tool for distributed event-based systems.In Proceedings of the ACM/IEEE International Conference on Software Engineering (ICSE Demo track).
David Lo and Shahar Maoz. Scenario-based and value-based specification mining: Better together. In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering (ASE).
loggly Retrieved from https://www.loggly.com/.
logstash Retrieved from https://www.elastic.co/products/logstash.
Davide Lorenzoli, Leonardo Mariani, and Mauro Pezzè. Automatic generation of software behavioral models. In Proceedings of the International Conference on Software Engineering (ICSE).
Stuart Marshall, Kirk Jackson, Craig Anslow, and Robert Biddle. Aspects to visualizing reusable components. In Proceedings of the Asia-Pacific Symposium on Information Visualization (APVis), Vol.
Davide Lorenzoli, Leonardo Mariani, and Mauro Pezzè. Automatic generation of software behavioral models. In Proceedings of the International Conference on Software Engineering (ICSE).
Stuart Marshall, Kirk Jackson, Craig Anslow, and Robert Biddle. Aspects to visualizing reusable components. In Proceedings of the Asia-Pacific Symposium on Information Visualization (APVis), Vol.
Stuart(SIGCOMM). [ ] Weiyi Shang, Meiyappan Nagappan, Ahmed E. Hassan, and Zhen Ming Jiang. Understanding log lines in development knowledge. In Proceedings of the International Conference on Software Maintenance and Evolution (ICSME). DOI: https://doi.org/10.1109/ICSME.2016.70. [ ] Benjamin H. Sigelman, Luiz André Barroso, Mike Burrows, Pat Stephenson, Manoj Plakal, Donald Beaver, Jaspan, and Chandan Shanbhag. Dapper, a Large-Scale Distributed Systems Tracing Infrastructure. Report. Google, Inc. Retrieved from http://research.google.com/archive/papers/dapper-large-scale-distributed-systems.pdf. [ ] Splunk Retrieved from http://www.splunk.com/. [ ] Roshan Sumbaly, Jay Kreps, Lei Gao, Alex Feinberg, Chinmay Soman, and Sam Shah. Serving large-scale computed data with Project Voldemort. In Proceedings of the USENIX Conference on File and Storage Technologies (FAST). ACM Transactions on Software Engineering and Methodology, Vol., No., Article Pub. date: March 2019. Steven P. Reiss. 2001. An overview of BLOOM. In Proceedings of the ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering (PASTE01). 25-31. DOI: https://doi.org/10.1145/379605.379629 Steven P. Reiss. 2003. JIVE: Visualizing Java in action demonstration description. In Proceedings of the ACM/IEEE International Conference on Software Engineering (ICSE03). 820-821. DOI: https://doi.org/10.1109/ICSE.2003.1201303 Steven P.ReissandManosRenieris.2001.Encodingprogramexecutions.InProceedingsoftheACM/IEEEInternational Roshan Sumbaly, Jay Kreps, Lei Gao, Alex Feinberg, Chinmay Soman, and Sam Shah. 2012. Serving large-scale batch computed data with Project Voldemort. In Proceedings of the USENIX Conference on File and Storage Technologies (FAST12). 18-18. ACM Transactions on Software Engineering and Methodology, Vol. 29, No. 2, Article 9. Pub. date: March 2019 [ ] Saeed Taheri, Ian Briggs, Martin Burtscher, and Ganesh Gopalakrishnan. DiffTrace: Efficient trace analysis and diffing for debugging. In Proceedings of the International Conference on Cluster Computing. [ ] Byung Chul Tak, Chunqiang Tang, Chun Zhang, Sriram Govindan, Bhuvan Urgaonkar, and Rong N. Chang. Path: Precise discovery of request processing paths from black-box observations of thread and network activities. In Proceedings of the USENIX Annual Technical Conference (USENIX). [ ] Jiaqi Tan, Xinghao Pan, Soila Kavulya, Rajeev Gandhi, and Priya Narasimhan. SALSA: analyzing logs as machines. In Proceedings of the 1st USENIX Conference on Analysis of System Logs (WASL). [ ] Jiaqi Tan, Xinghao Pan, Soila Kavulya, Rajeev Gandhi, and Priya Narasimhan. Mochi: Visual log-analysis tools for debugging Hadoop. In Proceedings of the USENIX Workshop on Hot Topics in Cloud Computing (HotCloud09). [ ] Jonas Trümper, Jürgen Döllner, and Alexandru Telea. Multiscale visual comparison of execution traces. Proceedings of the International Conference on Program Comprehension (ICPC). ICPC. [ ] Neil Walkinshaw and Kirill Bogdanov. Inferring finite-state models with temporal constraints. In Proceedings of the IEEE/ACM International Conference on Automated Software Engineering (ASE). [ ] Tianyin Xu, Han Min Naing, Le Lu, and Yuanyuan Zhou. How do system administrators resolve issues in the real world? In Proceedings of the Conference on Human Factors in Computing Systems (CHI). [ ] Tianyin Xu and Yuanyuan Zhou. Systems approaches to tackling configuration errors: A survey. Comput., (July), :DOI: https://doi.org/10.1093/comjnl/bxy063 [ ] Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael Jordan. Experience mining Google's production console logs. In Proceedings of the Workshop on Managing Systems via Log Analysis and Machine Learning Techniques (SLAML). [ ] Ding Yuan, Soyeon Park, and Yuanyuan Zhou. Characterizing logging practices in open-source software. Proceedings of the International Conference on Software